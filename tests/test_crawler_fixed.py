#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«ä»£ç 
"""

import sys
import os
import time

# æ·»åŠ src/mainç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src', 'main'))

def test_crawler():
    """æµ‹è¯•çˆ¬è™«åŠŸèƒ½"""
    try:
        from newsmth_GUI import favorite_posts_content_craw
        print("âœ… æˆåŠŸå¯¼å…¥ favorite_posts_content_craw ç±»")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = favorite_posts_content_craw()
        print("âœ… æˆåŠŸåˆ›å»ºçˆ¬è™«å®ä¾‹")
        
        # æµ‹è¯•URLï¼ˆè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹URLï¼Œä½ éœ€è¦æ›¿æ¢ä¸ºå®é™…çš„å¸–å­URLï¼‰
        test_url = "https://m.newsmth.net/article/JobExpress/single/5b4b4b4b/0"
        
        print(f"ğŸ”— æµ‹è¯•URL: {test_url}")
        print("="*50)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æµ‹è¯•çˆ¬å–åŠŸèƒ½
        result = crawler.fetch_content(test_url)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        elapsed_time = end_time - start_time
        
        if result:
            print("âœ… çˆ¬å–æˆåŠŸï¼")
            print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            print(f"ğŸ“‹ ä¸»é¢˜: {result.get('theme', 'N/A')}")
            print(f"ğŸ“ å¸–å­æ•°é‡: {len(result.get('posts', []))}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªå¸–å­çš„ä¿¡æ¯
            for i, post in enumerate(result.get('posts', [])[:3]):
                print(f"\nğŸ“„ å¸–å­ {i+1}:")
                print(f"   ğŸ·ï¸  ç±»å‹: {post.get('type', 'N/A')}")
                print(f"   ğŸ“Œ æ ‡é¢˜: {post.get('title', 'N/A')}")
                print(f"   ğŸ’¬ å†…å®¹é•¿åº¦: {len(post.get('content', ''))}")
                print(f"   ğŸ–¼ï¸  å›¾ç‰‡æ•°é‡: {len(post.get('images', []))}")
                
                # æ˜¾ç¤ºå›¾ç‰‡ä¿¡æ¯
                if post.get('images'):
                    for j, img in enumerate(post['images'][:2]):  # åªæ˜¾ç¤ºå‰2å¼ å›¾ç‰‡
                        print(f"     ğŸ–¼ï¸  å›¾ç‰‡ {j+1}: {img.get('url', 'N/A')}")
                        print(f"        ğŸ“Š æ•°æ®å¤§å°: {len(img.get('data', b'')) if img.get('data') else 'None'} bytes")
        else:
            print("âŒ çˆ¬å–å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½...")
    
    try:
        import requests
        from bs4 import BeautifulSoup
        
        # æµ‹è¯•ç½‘ç»œè¿æ¥
        url = "https://m.newsmth.net/"
        headers = {
            "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0"
        }
        
        print(f"ğŸŒ æµ‹è¯•ç½‘ç»œè¿æ¥: {url}")
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            print(f"âœ… ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ŒçŠ¶æ€ç : {response.status_code}")
            print(f"ğŸ“„ é¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.text, "html.parser")
            title = soup.find("title")
            if title:
                print(f"ğŸ“‹ é¡µé¢æ ‡é¢˜: {title.get_text(strip=True)}")
            
            # æŸ¥æ‰¾é¡µé¢ä¿¡æ¯å…ƒç´ 
            page_info = soup.find("a", class_="plant")
            if page_info:
                print(f"ğŸ“Š æ‰¾åˆ°é¡µé¢ä¿¡æ¯å…ƒç´ : {page_info.get_text(strip=True)}")
            else:
                print("âš ï¸  æœªæ‰¾åˆ°é¡µé¢ä¿¡æ¯å…ƒç´ ")
                
        else:
            print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¿®å¤åçš„çˆ¬è™«ä»£ç ")
    print("="*60)
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    test_basic_functionality()
    
    print("\n" + "="*60)
    
    # æµ‹è¯•çˆ¬è™«åŠŸèƒ½
    test_crawler()
    
    print("\n" + "="*60)
    print("ğŸ¯ æµ‹è¯•å®Œæˆï¼")

